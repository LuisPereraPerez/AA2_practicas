{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\LUIS PP\\ULPGC\\TERCERO\\AA2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "109IpZRc_Q9F",
        "outputId": "79356bbe-c9a7-4f02-8ad6-3f2e94b6a917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clases encontradas: ['Austria', 'Belgium', 'Bulgaria', 'Croatia', 'Czech Republic', 'Denmark', 'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Holland', 'Hungary', 'Ireland', 'Italy', 'Latvia', 'Lithuania', 'Luxembourg', 'Malta', 'Slovakia', 'Slovenia', 'South Cyprus', 'Spain', 'Sweden']\n",
            "Número de clases: 24\n",
            "Tamaño del conjunto de entrenamiento: 796\n",
            "Tamaño del conjunto de prueba: 200\n"
          ]
        }
      ],
      "source": [
        "# Definir las transformaciones para las imágenes\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Redimensionar las imágenes a 128x128\n",
        "    transforms.ToTensor(),          # Convertir las imágenes a tensores\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalizar con media 0.5 y desviación estándar 0.5\n",
        "])\n",
        "\n",
        "# Especificar la ruta del dataset\n",
        "dataset_path = 'Flags'  # Cambia esto a la ruta donde descargaste el dataset\n",
        "\n",
        "# Cargar el dataset de imágenes\n",
        "train_data = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
        "\n",
        "# Calcular el tamaño total del conjunto de datos\n",
        "dataset_size = len(train_data)\n",
        "\n",
        "# Definir las proporciones para dividir el dataset en entrenamiento y prueba\n",
        "train_size = int(0.8 * dataset_size)\n",
        "test_size = dataset_size - train_size\n",
        "\n",
        "# Dividir el dataset en subconjuntos de entrenamiento y prueba\n",
        "train_dataset, test_dataset = random_split(train_data, [train_size, test_size])\n",
        "\n",
        "# Crear DataLoaders para los conjuntos de entrenamiento y prueba\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Obtener los nombres de las clases\n",
        "class_names = train_data.classes\n",
        "\n",
        "# Imprimir información sobre el conjunto de datos\n",
        "print(f'Clases encontradas: {class_names}')\n",
        "print(f'Número de clases: {len(class_names)}')\n",
        "print(f'Tamaño del conjunto de entrenamiento: {len(train_loader.dataset)}')\n",
        "print(f'Tamaño del conjunto de prueba: {len(test_loader.dataset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oNaYAa7p_Q9H"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        # Capa de convolución 1: Entrada de 3 canales (RGB), salida de 16 canales\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
        "\n",
        "        # Max Pooling: Kernel 2x2\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "        # Capa de convolución 2: Entrada de 16 canales, salida de 32 canales\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
        "\n",
        "        # Capa totalmente conectada 1: Después de dos capas de convolución y pooling, el tamaño es 32x32\n",
        "        # Imágenes de entrada de 128x128 reducidas a 32x32 por el pooling\n",
        "        self.fc1 = nn.Linear(32 * 32 * 32, 120)\n",
        "\n",
        "        # Capa totalmente conectada 2: Reducción a 24 salidas para las 3 clases (Rock, Paper, Scissors)\n",
        "        self.fc2 = nn.Linear(120, 24)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Primera capa de convolución seguida de ReLU y pooling\n",
        "        x = F.relu(self.conv1(x))  # Activación ReLU después de la convolución\n",
        "        x = self.pool(x)  # Pooling\n",
        "\n",
        "        # Segunda capa de convolución seguida de ReLU y pooling\n",
        "        x = F.relu(self.conv2(x))  # Activación ReLU después de la convolución\n",
        "        x = self.pool(x)  # Pooling\n",
        "\n",
        "        # Aplanar la salida para pasar a la capa totalmente conectada\n",
        "        x = x.view(-1, 32 * 32 * 32)  # Aplanamos la salida\n",
        "\n",
        "        # Primera capa totalmente conectada\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        # Segunda capa totalmente conectada (capa de salida)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qZjbvLGX_Q9J",
        "outputId": "b7a98f4c-de97-4898-86e0-e077cedd12d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 2.7526\n",
            "Epoch 2, Loss: 1.3150\n",
            "Epoch 3, Loss: 0.7107\n",
            "Epoch 4, Loss: 0.3958\n",
            "Epoch 5, Loss: 0.2423\n"
          ]
        }
      ],
      "source": [
        "# Inicializar el modelo, la función de pérdida y el optimizador\n",
        "model = ConvNet()  # Instanciamos el modelo de red neuronal\n",
        "criterion = nn.CrossEntropyLoss()  # Usamos la pérdida de entropía cruzada para la clasificación multi-clase\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Optimizador Adam con tasa de aprendizaje de 0.001\n",
        "\n",
        "# Definir el número de épocas\n",
        "epochs = 5\n",
        "\n",
        "# Entrenamiento\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0  # Inicializamos la pérdida acumulada de la época\n",
        "\n",
        "    # Iterar sobre el conjunto de entrenamiento\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()  # Limpiar los gradientes previos para no acumularlos\n",
        "\n",
        "        # Pasamos las imágenes por el modelo\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculamos la pérdida entre las predicciones y las etiquetas\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backpropagation: calcular los gradientes de la pérdida con respecto a los parámetros del modelo\n",
        "        loss.backward()\n",
        "\n",
        "        # Actualizamos los pesos del modelo con el optimizador\n",
        "        optimizer.step()\n",
        "\n",
        "        # Acumulamos la pérdida para esta iteración\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Promedio de la pérdida por época y mostrar el progreso\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sVNZkldD_Q9M",
        "outputId": "072e7cd4-0622-491e-9c27-81949acff510"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the model on the test images: 82.50%\n"
          ]
        }
      ],
      "source": [
        "# Variables para almacenar los resultados\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Evaluación del modelo sin calcular gradientes\n",
        "with torch.no_grad():  # Deshabilita el cálculo de gradientes, útil para la evaluación\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)  # Pasamos las imágenes por el modelo\n",
        "        _, predicted = torch.max(outputs.data, 1)  # Obtenemos las predicciones (clase con mayor probabilidad)\n",
        "        \n",
        "        total += labels.size(0)  # Incrementamos el total de imágenes evaluadas\n",
        "        correct += (predicted == labels).sum().item()  # Incrementamos el número de predicciones correctas\n",
        "\n",
        "# Calculamos la precisión y la mostramos\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy of the model on the test images: {accuracy:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "81KHIsOq_Q9P",
        "outputId": "293a2fbe-9c8b-494c-cb6f-18a326d867dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-12-26 16:47:25,567] A new study created in memory with name: no-name-03405a99-d61d-4cef-bb35-3cbf76e44752\n",
            "C:\\Users\\INES\\AppData\\Local\\Temp\\ipykernel_10856\\1064966657.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
            "[I 2024-12-26 16:48:31,428] Trial 0 finished with value: 0.765 and parameters: {'lr': 0.0015744600836828603, 'batch_size': 112}. Best is trial 0 with value: 0.765.\n",
            "[I 2024-12-26 16:49:35,407] Trial 1 finished with value: 0.055 and parameters: {'lr': 0.006997943703606682, 'batch_size': 32}. Best is trial 0 with value: 0.765.\n",
            "[I 2024-12-26 16:50:41,295] Trial 2 finished with value: 0.86 and parameters: {'lr': 0.0008729529045138773, 'batch_size': 48}. Best is trial 2 with value: 0.86.\n",
            "[I 2024-12-26 16:51:52,224] Trial 3 finished with value: 0.51 and parameters: {'lr': 4.965935073691217e-05, 'batch_size': 80}. Best is trial 2 with value: 0.86.\n",
            "[I 2024-12-26 16:52:59,991] Trial 4 finished with value: 0.83 and parameters: {'lr': 0.0022769442805763347, 'batch_size': 96}. Best is trial 2 with value: 0.86.\n",
            "[I 2024-12-26 16:54:09,034] Trial 5 finished with value: 0.055 and parameters: {'lr': 0.00907766803618318, 'batch_size': 48}. Best is trial 2 with value: 0.86.\n",
            "[I 2024-12-26 16:55:15,406] Trial 6 finished with value: 0.685 and parameters: {'lr': 0.00351973243531566, 'batch_size': 112}. Best is trial 2 with value: 0.86.\n"
          ]
        }
      ],
      "source": [
        "# Función para definir el modelo\n",
        "def create_model(trial):\n",
        "    model = ConvNet()\n",
        "\n",
        "    # Hiperparámetros que se ajustarán\n",
        "    learning_rate = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    return model, optimizer\n",
        "\n",
        "# Función de entrenamiento\n",
        "def train_model(trial):\n",
        "    model, optimizer = create_model(trial)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Hiperparámetros\n",
        "    batch_size = trial.suggest_int('batch_size', 16, 128, step=16)  # Tamaño de lote\n",
        "    epochs = 5\n",
        "\n",
        "    # Crear DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    # Evaluar el modelo\n",
        "    accuracy = evaluate_model(model, test_loader)\n",
        "\n",
        "    return accuracy  # Devuelve la métrica que se quiere optimizar\n",
        "\n",
        "# Función para evaluar el modelo\n",
        "def evaluate_model(model, test_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return correct / total  # Devuelve la precisión\n",
        "\n",
        "# Crear un estudio y optimizar\n",
        "study = optuna.create_study(direction='maximize')  # Queremos maximizar la precisión\n",
        "study.optimize(train_model, n_trials=7)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AA2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
